{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete\n"
     ]
    }
   ],
   "source": [
    "# Making necessary imports\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Declaring a dictioray with links to confirmed, recovered and deaths datasets\n",
    "urls = {\n",
    "    'confirmed' : \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv&filename=time_series_covid19_confirmed_global.csv'\",\n",
    "    'recovery' : \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_recovered_global.csv&filename=time_series_covid19_recovered_global.csv\",\n",
    "    'death' : \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_deaths_global.csv&filename=time_series_covid19_deaths_global.csv\",\n",
    "    'testing' : \"https://covid.ourworldindata.org/data/owid-covid-data.xlsx\"\n",
    "}\n",
    "\n",
    "# Making a function that can download and save the dataset\n",
    "save_link = \"C:\\\\Users\\\\usama\\\\OneDrive\\\\Desktop\\\\github\\\\COVID19-visualization\\\\dataset\\\\\"  #path to dataset folder\n",
    "def download_files(url):\n",
    "    dlink = requests.get(urls[url])\n",
    "    fname = url + \".csv\"\n",
    "    data = open(os.path.join(save_link, fname), 'wb').write(dlink.content)\n",
    "\n",
    "# Running a for loop to download all files\n",
    "for url in urls:\n",
    "    download_files(url)\n",
    "\n",
    "# Coverting \"Testing\" file from xls to csv\n",
    "testing_xls = pd.read_excel('./dataset/testing.csv', 'Sheet1', dtype=str, index_col=None)\n",
    "testing_xls.to_csv('./dataset/testing.csv', encoding='utf-8', index=False)\n",
    "#data_xls.to_csv('csvfile.csv', encoding='utf-8', index=False)\n",
    "\n",
    "print('Download complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv(\"./dataset/testing.csv\")\n",
    "unique = testing['location'].unique()\n",
    "\n",
    "dicts = []\n",
    "\n",
    "for country in unique:\n",
    "    count = testing[testing['location'] == country].count()[2]\n",
    "    new_dict = { 'Country' : country, 'Counts' : count}\n",
    "    dicts.append(new_dict)\n",
    "    \n",
    "data = pd.DataFrame(dicts)\n",
    "slicer = data[data['Counts'] == data['Counts'].max()]\n",
    "cols = testing[testing['location'] == slicer.iloc[0][0]]['date'].to_list()\n",
    "\n",
    "reframe = pd.DataFrame(columns = [cols])\n",
    "\n",
    "n=0\n",
    "for country in unique:\n",
    "    zeros = len(cols) - testing[testing['location'] == country]['date'].count()\n",
    "    test = testing[testing['location'] == country]['new_tests']\n",
    "    row = []\n",
    "    for x in range(0,zeros):\n",
    "        row.append(0)\n",
    "    row.extend(test)\n",
    "    reframe.loc[n] = row\n",
    "    n = n + 1\n",
    "    \n",
    "unique = pd.DataFrame(unique, columns = ['Country'])\n",
    "reframe = pd.concat([unique,reframe], axis = 1)\n",
    "cols.insert(0, \"Country\")\n",
    "reframe.columns = [cols]\n",
    "reframe.fillna(0, inplace=True)\n",
    "reframe.to_csv('./outputs/testing_all.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['confirmed.csv', 'death.csv', 'recovery.csv', 'testing.csv']"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_countries(lst):\n",
    "    arr = []\n",
    "    \n",
    "    for file in lst:\n",
    "        arr.append(pd.read_csv(os.path.join('./outputs',file)).shape[0])\n",
    "        \n",
    "    result = {'File' : lst , 'Counts': arr}\n",
    "    result = pd.DataFrame(result)\n",
    "    file = result[result['Counts'] == result['Counts'].min()].iloc[0][0]\n",
    "    unique = pd.read_csv(os.path.join('./outputs',file))['Country']\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Afghanistan\n",
       "1                    Albania\n",
       "2                    Algeria\n",
       "3                    Andorra\n",
       "4                     Angola\n",
       "               ...          \n",
       "182           Western Sahara\n",
       "183    Sao Tome and Principe\n",
       "184                    Yemen\n",
       "185                  Comoros\n",
       "186               Tajikistan\n",
       "Name: Country, Length: 187, dtype: object"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = unique_countries(os.listdir('./outputs'))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency(unique, fnames):\n",
    "    for file in fnames:\n",
    "        dataF = pd.read_csv(os.path.join('./outputs',file))\n",
    "        n_dataset = pd.DataFrame()\n",
    "        for country in unique:\n",
    "            sli = da[da['Country'] == country]\n",
    "            n_dataset = pd.concat([n_dataset, sli], axis=0)\n",
    "        n_dataset.to_csv('./all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency(a, os.listdir('./outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows():\n",
    "    files = os.listdir('./outputs')\n",
    "    row = []\n",
    "    for file in files:\n",
    "        dat = pd.read_csv(os.path.join('./outputs',file))\n",
    "        row.append(dat.shape[1])\n",
    "    return np.array(row).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./outputs/testing_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-104"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((rows()-1)*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,((rows()-1)*-1):-1]\n",
    "data.iloc[:,0]\n",
    "tes = pd.concat([data.iloc[:,0], data.iloc[:,((rows()-1)*-1):-1]], axis=1)\n",
    "tes.to_csv('./ref_testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class data_preprocessing():\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.datasetf = './dataset'\n",
    "        self.outputf = './outputs'\n",
    "        self.path = os.path.join(self.datasetf,filename)\n",
    "        self.dataset = pd.read_csv(self.path)\n",
    "\n",
    "    def merge_states(self):\n",
    "        datafr = self.dataset\n",
    "        new_dict = []\n",
    "        for country in self.unique:\n",
    "            c_data = datafr[datafr['Country/Region'] == country].sum(axis=0)\n",
    "            new_dict.append(c_data)\n",
    "        d = pd.DataFrame(new_dict)\n",
    "        d = d.drop(['Province/State', 'Country/Region', 'Lat', 'Long'], axis = 1)\n",
    "        unique = pd.DataFrame(self.unique, columns = ['Country'])\n",
    "        dataset = pd.concat([unique, d], axis = 1)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def re_dimension_testing(self):\n",
    "        filepath = './dataset/testing.csv'\n",
    "        d = pd.read_csv(filepath)\n",
    "        unique = d['location'].unique()\n",
    "        dicts = []\n",
    "        for country in unique:\n",
    "            count = d[d['location'] == country].count()[2]\n",
    "            new_dict = { 'Country' : country, 'Counts' : count}\n",
    "            dicts.append(new_dict)\n",
    "        data = pd.DataFrame(dicts)\n",
    "        slicer = data[data['Counts'] == data['Counts'].max()]\n",
    "        cols = d[d['location'] == slicer.iloc[0][0]]['date'].to_list()\n",
    "        reframe = pd.DataFrame(columns = [cols])\n",
    "        n=0\n",
    "        for country in unique:\n",
    "            zeros = len(cols) - d[d['location'] == country]['date'].count()\n",
    "            testing = d[d['location'] == country]['new_tests']\n",
    "            row = []\n",
    "            for x in range(0,zeros):\n",
    "                row.append(0)\n",
    "            row.extend(testing)\n",
    "            reframe.loc[n] = row\n",
    "            n = n + 1\n",
    "        unique = pd.DataFrame(unique, columns = ['Country'])\n",
    "        reframe = pd.concat([unique,reframe], axis = 1)\n",
    "        cols.insert(0, \"Country\")\n",
    "        reframe.columns = [cols]\n",
    "        reframe.fillna(0, inplace=True)\n",
    "        self.dataset = reframe\n",
    "        \n",
    "    def consistent_countries_t(self):\n",
    "        dataset = pd.read_csv('./test/testing.csv')\n",
    "        unique = pd.DataFrame(self.unique)\n",
    "        unique.columns = ['Country']\n",
    "        df3 = pd.merge(unique, dataset, on='Country', how = 'left')\n",
    "        df3.fillna(0, inplace=True)\n",
    "        self.dataset = df3\n",
    "        \n",
    "    def consistent_countries(self):\n",
    "        dataFr = self.dataset\n",
    "        unique = pd.DataFrame(self.unique)\n",
    "        unique.columns = ['Country']\n",
    "        df3 = pd.merge(unique, dataFr, on='Country', how = 'left')\n",
    "        self.dataset = df3 \n",
    "        \n",
    "    def consistent_dates(self):\n",
    "        data = self.dataset\n",
    "        data.iloc[:,((self.row-1)*-1):-1]\n",
    "        data.iloc[:,0]\n",
    "        tes = pd.concat([data.iloc[:,0], data.iloc[:,((self.row-1)*-1):-1]], axis=1)\n",
    "        self.dataset = tes\n",
    "\n",
    "    def save_file(self):\n",
    "        save_path = os.path.join('./test',self.filename)\n",
    "        dataset = pd.DataFrame(self.dataset)\n",
    "        dataset.to_csv(save_path, index=False)\n",
    "        \n",
    "    @classmethod\n",
    "    def time_rows(cls, datasetf):\n",
    "        files = os.listdir(datasetf)\n",
    "        row = []\n",
    "        for file in files:\n",
    "            dat = pd.read_csv(os.path.join(datasetf, file))\n",
    "            row.append(dat.shape[1])\n",
    "        data_preprocessing.row = np.array(row)[0] - 3\n",
    "        \n",
    "    @classmethod\n",
    "    def unique_countries(cls, datasetf):\n",
    "        lst = os.listdir(datasetf)\n",
    "        arr = []\n",
    "        for file in lst:\n",
    "            arr.append(pd.read_csv(os.path.join(datasetf,file)).shape[0])\n",
    "        result = {'File' : lst , 'Counts': arr}\n",
    "        result = pd.DataFrame(result)\n",
    "        file = result[result['Counts'] == result['Counts'].min()].iloc[0][0]\n",
    "        unique = pd.read_csv(os.path.join(datasetf,file))['Country/Region'].unique()\n",
    "        data_preprocessing.unique = unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "const2 = data_preprocessing('testing.csv')\n",
    "const2.time_rows(const2.datasetf)\n",
    "const2.unique_countries(const2.datasetf)\n",
    "const2.re_dimension_testing()\n",
    "const2.save_file()\n",
    "const2.consistent_countries_t()\n",
    "const2.consistent_dates()\n",
    "const2.save_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 104)\n",
      "(187, 104)\n",
      "(187, 104)\n"
     ]
    }
   ],
   "source": [
    "files = ['confirmed.csv' , 'death.csv', 'recovery.csv']\n",
    "\n",
    "for file in files:\n",
    "    const = data_preprocessing(file)\n",
    "    const.time_rows(const.datasetf)\n",
    "    const.unique_countries(const.datasetf)\n",
    "    const.merge_states()\n",
    "    const.consistent_countries()\n",
    "    const.consistent_dates()\n",
    "    const.save_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Covid19",
   "language": "python",
   "name": "covid19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
